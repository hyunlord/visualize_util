# LLM Provider Configuration
LLM_PROVIDER=openrouter          # claude | openai | ollama | openrouter
LLM_MODEL=anthropic/claude-sonnet-4-5-20250929
LLM_API_KEY=
LLM_BASE_URL=                    # Custom endpoint (Ollama: http://localhost:11434)

# Application
HOST=0.0.0.0
PORT=8000
DEBUG=true
DATA_DIR=./data
